{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/koki0702/dezero-book/blob/master/en/step07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD_v5BgmSNeK"
   },
   "source": [
    "**The code implemented in the previous step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1ItduEGlSNeK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "class Function:\n",
    "    def __call__(self, input):\n",
    "        x = input.data\n",
    "        y = self.forward(x)\n",
    "        output = Variable(y)\n",
    "        self.input = input\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, gy):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Square(Function):\n",
    "    def forward(self, x):\n",
    "        y = x ** 2\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.input.data\n",
    "        gx = 2 * x * gy\n",
    "        return gx\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.exp(x)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.input.data\n",
    "        gx = np.exp(x) * gy\n",
    "        return gx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b64KOJJASNeO"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP5kwa-3-9Ff"
   },
   "source": [
    "# Step 7: Automation of back-propagation\n",
    "\n",
    "In the previous step, we successfully moved the back-propagation. However, there we had to manually code the back propagation calculations. What this means is that every time we do a new calculation, we have to write the code for back-propagation in our own hands. For example, if you consider some computational graphs like the one in **Figure 7-1**, you would have to write the code for backward propagation in each computation by hand. It is a tedious task above all else, with the potential for mistakes. Let's automate the Boring Stuff with Python!\n",
    "\n",
    "<br>![img1-14](images/1-14.png)\n",
    "\n",
    "**Figure 7-1** Examples of various computation graphs (variable names are omitted and functions are written in class names)<br><br>\n",
    "\n",
    "The next step is the automation of backward propagation. To be more precise, the idea is to create a mechanism by which the normal computation (forward propagation) - whatever computation it is - will automatically do the backward propagation. This is where Define-by-Run gets to the heart of the matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>NOTE</b>\n",
    "\n",
    "Define-by-Run is a mechanism for making \"connections\" between calculations made in deep learning at the timing of the calculation. This is also referred to as a \"dynamic computational graph\", and you can learn more about Define-by-Run and its benefits in the \"column Define-by-Run\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the calculation graphs shown in **Figure 7-1** are all aligned calculations. Therefore, if we keep the order of the functions in the form of a list, we can do the backward propagation automatically by following them in the opposite direction. However, for branching graphs or complex graphs in which variables are used more than once, the simple list method cannot be used. Our goal is to build a system that can automatically backward propagate even the most complex computational graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>WARNING</b> \n",
    "\n",
    "As a matter of fact, it is also possible to perform back-propagation correctly for any computation graph by simply adding the computations performed to the list if the data structure of the list is modified. That data structure is called a Wengert list (or \"tape\" as it is also called). This book does not describe the Wengert list; those interested in the Wengert list should read the literature <a href=\"https://dl.acm.org/doi/10.1145/355586.364791\">[2]</a> <a href=\"http://www.cs.cmu.edu/~wcohen/10-605/notes/autodiff.pdf\">[3]</a> and others. Also, the benefits of Define-by-Run for Wengert lists can be found in literature <a href=\"https://openreview.net/pdf?id=BJJsrmfCZ\">[4]</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 For the automation of back propagation\n",
    "\n",
    "In automating backward propagation, we start by thinking about the relationship between variables and functions. First, let's look at it from the perspective of a function, that is, \"What does a variable look like from a function? From the perspective of the function, variables exist as \"input\" and \"output\". A variable exists for a function as an \"input variable (`input`)\" and an \"output variable (`output`)\" (the dashed line in the figure shows the reference), as shown in the left figure of **Figure 7-2**.\n",
    "\n",
    "<br>![img1-15](images/1-15.png)\n",
    "\n",
    "**Figure 7-2** Relationships between variables as seen from functions (left) and functions as seen from variables (right)<br><br>\n",
    "\n",
    "Moving on, what does a function look like from the perspective of a variable? The point to note here is that variables are \"created\" by functions. In other words, the function is the \"creator\" for the variable. In other words, it is a `creator` (creator). If the creator function does not exist, then it is considered to be a variable created by something other than the function, such as a variable given by the user.\n",
    "\n",
    "Now, let's incorporate the function-variable \"connections\" represented in **Figure 7-2** into our code. Here, we will make that \"connection\" at the very moment when the normal computation (forward propagation) takes place. To do so, we first add the following code to the `Variable` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.creator = None       # Added code\n",
    "\n",
    "    def set_creator(self, func):  # Added code\n",
    "        self.creator = func       # Added code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we add an instance variable called `creator`. Then add a method to set the `creator` as the `set_creator` method. Then, add the following code to the `Function` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def __call__(self, input):\n",
    "        x = input.data\n",
    "        y = self.forward(x)\n",
    "        output = Variable(y)\n",
    "        output.set_creator(self)  # Let the output variable remember its creator\n",
    "        self.input = input\n",
    "        self.output = output      # Remember the output\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, gy):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward calculation creates a `Variable` instance of `output`. At this time, the generated `output` is made to remember that \"I (myself as a function) am the creator\". This is the heart of the mechanism for creating dynamic connections. Here, we set the output to the instance variable `output` to prepare for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>NOTE</b>\n",
    "\n",
    "DeZero's dynamic computation graphs work by recording their connections in a \"box\" called a variable when the actual calculation is done. A similar approach has been taken with Chainer and PyTorch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, if we have `Variable` and `Function` with connections, we can follow the computation graph in the opposite direction. In concrete code, it looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(Function):\n",
    "    def forward(self, x):\n",
    "        y = x ** 2\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.input.data\n",
    "        gx = 2 * x * gy\n",
    "        return gx\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def forward(self, x):\n",
    "        y = np.exp(x)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x = self.input.data\n",
    "        gx = np.exp(x) * gy\n",
    "        return gx\n",
    "\n",
    "\n",
    "A = Square()\n",
    "B = Exp()\n",
    "C = Square()\n",
    "\n",
    "x = Variable(np.array(0.5))\n",
    "a = A(x)\n",
    "b = B(a)\n",
    "y = C(b)\n",
    "\n",
    "# Trace a node in a computational graph in reverse\n",
    "assert y.creator == C\n",
    "assert y.creator.input == b\n",
    "assert y.creator.input.creator == B\n",
    "assert y.creator.input.creator.input == a\n",
    "assert y.creator.input.creator.input.creator == A\n",
    "assert y.creator.input.creator.input.creator.input == x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will explain about the assert statement, which is used like `assert ...`. Here, if `...` is not `True` it throws exception. Therefore, the assert statement can be used to check if the condition is met. By the way, if you run the above code, you can see that it satisfies all the conditions of the assert statement because it runs without any problems (no exceptions are raised).\n",
    "\n",
    "As the above code shows, the instance variable `creator` of `Variable` takes you to the previous `Function`. And the instance variable `input` of `Function` takes you to the previous `Variable`. If we were to diagram this connection, it would look like **Figure 7-3**.\n",
    "\n",
    "<br>![img1-16](images/1-16.png)\n",
    "\n",
    "**Fig. 7-3** The computational graph of the starting point of `y` in the reverse direction.<br><br>\n",
    "\n",
    "**As shown in Figure 7-3**, our computational graph is constructed by the connections between functions and variables. And, importantly, that connection is made when the computation actually takes place (when the data flows in forward propagation). This feature is what Define-by-Run is all about. In other words, a connection is created by the flow of data.\n",
    "\n",
    "A data structure with connections as shown in **Figure 7-3** is called a \"linked node\". Nodes are the elements that make up a graph, and links represent references to different nodes. In other words, we are representing a computational graph in terms of a data structure called \"linked nodes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Trying backward Propagation\n",
    "\n",
    "Now, let's do some backward propagation using the connection between variables and functions. First, backward propagation from `y` to `b` is performed. This can be implemented as follows (along with a diagram for your reference)\n",
    "\n",
    "<br>![img1-161](images/1-161.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.grad = np.array(1.0)\n",
    "\n",
    "C = y.creator  # 1. Get a function\n",
    "b = C.input  # 2. Get the input of the function\n",
    "b.grad = C.backward(y.grad)  # 3. Call the backward method of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get a function from the instance variable `creator` of `y` and the input variable by the `input` of that function. Then it calls the `backward` method of the function. Following this, here is a diagram and code for the back propagation from the variable `b` to `a`.\n",
    "\n",
    "<br>![img1-162](images/1-162.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = b.creator  # 1. Get a function\n",
    "a = B.input  # 2. Get the input of the function\n",
    "a.grad = B.backward(b.grad)  # 3. Call the backward method of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, too, we use the same logic as before for reverse propagation. Specifically.\n",
    "\n",
    " 1. get a function\n",
    " 2. get the input of the function\n",
    " 3. call the `backward` method of a function.\n",
    "\n",
    "\n",
    "This is the flow. And finally, the backward propagation from the variable `a` to `x`.\n",
    "\n",
    "<br>![img1-163](images/1-163.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.297442541400256\n"
     ]
    }
   ],
   "source": [
    "A = a.creator  # 1.Get a function\n",
    "x = A.input  # 2. Get input for the function\n",
    "x.grad = A.backward(a.grad)  # 3. Call the backward method of the function\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That concludes all the backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Adding the backward method\n",
    "\n",
    "As you may have already guessed, the backward propagation code I just showed shows the same processing flow repeatedly. More precisely, the logic of backward propagation from one variable back to the previous one is all the same. So we add a new method called `backward` to the `Variable` class so that we can automate the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.creator = None\n",
    "\n",
    "    def set_creator(self, func):\n",
    "        self.creator = func\n",
    "\n",
    "    def backward(self):\n",
    "        f = self.creator  # 1.Get a function\n",
    "        if f is not None:\n",
    "            x = f.input  # 2. Get input for the function\n",
    "            x.grad = f.backward(self.grad)  # 3. Call the backward method of the function\n",
    "            x.backward()  # Calling the backward method of the variable that is one step ahead of you (recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward` method is almost the same as the processing flow that has appeared repeatedly so far. Concretely, get a function from the `creator` of `Variable` and get the input variable of the function. Then it calls the `backward` method of the function. Finally, you call the variable's `backward` method on a variable that is one step ahead of the variable. This causes the `backward` method of each variable to be called recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>NOTE</b>\n",
    "\n",
    "If the <code>Variable</code> instance's <code>creator</code> is <code>None</code>, the backward propagation stops there. In that case, the <code>Variable</code> instance implies that it was created in a non-functional way -- that it is primarily a variable given by a user.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this new `Variable` to do some automated back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.297442541400256\n"
     ]
    }
   ],
   "source": [
    "A = Square()\n",
    "B = Exp()\n",
    "C = Square()\n",
    "\n",
    "x = Variable(np.array(0.5))\n",
    "a = A(x)\n",
    "b = B(a)\n",
    "y = C(b)\n",
    "\n",
    "y.grad = np.array(1.0)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `backward` method of the variable `y`, as described above, will cause automatic backward propagation. The results you get from doing it are the same as ever. Congratulations! This completes the most important base of automatic differentiation in DeZero."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "step07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
